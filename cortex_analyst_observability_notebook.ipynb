{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Observability for Cortex Analyst with Semantic Views\n",
    "\n",
    "This notebook registers a Cortex Analyst application to Snowflake's native AI Observability using TruLens with OTel-style instrumentation.\n",
    "\n",
    "**Features:**\n",
    "1. Setup & Configuration\n",
    "2. Cortex Analyst App Class\n",
    "3. TruLens Evaluation Setup\n",
    "4. Load Test Dataset from Snowflake\n",
    "5. Run Evaluation\n",
    "6. View Results\n",
    "\n",
    "**Required Packages:** `trulens-core`, `trulens-connectors-snowflake`, `trulens-providers-cortex`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n",
    "\n",
    "**Update the values below to match your environment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import List, Optional\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "session = get_active_session()\n",
    "print(\"Snowflake Notebook session active\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# USER CONFIGURATION - Update these values\n",
    "# =============================================================================\n",
    "\n",
    "# Semantic View to evaluate\n",
    "SEMANTIC_VIEW = \"<DATABASE>.<SCHEMA>.<SEMANTIC_VIEW_NAME>\"  # e.g., \"HAEBI_DEMO.HAEBI_SCHEMA.DUMMY_SEMANTIC_VIEW\"\n",
    "\n",
    "# Test Dataset Location (must have columns: INPUT_QUERY, OUTPUT_SQL)\n",
    "TEST_DATASET_DATABASE = \"<DATABASE>\"      # e.g., \"HAEBI_DEMO\"\n",
    "TEST_DATASET_SCHEMA = \"<SCHEMA>\"          # e.g., \"HAEBI_SCHEMA\"\n",
    "TEST_DATASET_TABLE = \"<TABLE_NAME>\"       # e.g., \"CORTEX_ANALYST_TEST_DATA\"\n",
    "\n",
    "# LLM Configuration\n",
    "JUDGE_MODEL = \"mistral-large2\"\n",
    "SUMMARIZATION_MODEL = \"mistral-large2\"\n",
    "\n",
    "# =============================================================================\n",
    "# Derived values (no need to change)\n",
    "# =============================================================================\n",
    "TEST_DATASET_FQN = f\"{TEST_DATASET_DATABASE}.{TEST_DATASET_SCHEMA}.{TEST_DATASET_TABLE}\"\n",
    "\n",
    "# API Endpoints (for internal Snowflake API)\n",
    "ANALYST_API_ENDPOINT = \"/api/v2/cortex/analyst/message\"\n",
    "COMPLETE_API_ENDPOINT = \"/api/v2/cortex/inference:complete\"\n",
    "\n",
    "print(f\"Semantic View: {SEMANTIC_VIEW}\")\n",
    "print(f\"Test Dataset: {TEST_DATASET_FQN}\")\n",
    "print(f\"Judge Model: {JUDGE_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cortex Analyst Application Class\n",
    "\n",
    "Instrumented for Snowflake AI Observability using OTel-style SpanAttributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _snowflake\n",
    "import re\n",
    "\n",
    "from trulens.core.otel.instrument import instrument\n",
    "from trulens.otel.semconv.trace import SpanAttributes\n",
    "\n",
    "\n",
    "class CortexAnalystApp:\n",
    "    \"\"\"\n",
    "    Cortex Analyst application instrumented for Snowflake AI Observability.\n",
    "    Uses _snowflake.send_snow_api_request() for internal API calls.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, session, semantic_view: str):\n",
    "        self.session = session\n",
    "        self.semantic_view = semantic_view\n",
    "        self.messages = []  # Conversation history for multi-turn\n",
    "    \n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
    "        attributes={\n",
    "            SpanAttributes.RETRIEVAL.QUERY_TEXT: \"query\",\n",
    "            SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: \"return\",\n",
    "        }\n",
    "    )\n",
    "    def call_cortex_analyst(self, query: str) -> dict:\n",
    "        \"\"\"\n",
    "        Call Cortex Analyst API with semantic view to get SQL interpretation.\n",
    "        This is the RETRIEVAL step - converting natural language to SQL.\n",
    "        \"\"\"\n",
    "        # Add user message to conversation\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": query}]\n",
    "        })\n",
    "        \n",
    "        request_body = {\n",
    "            \"messages\": self.messages,\n",
    "            \"semantic_view\": self.semantic_view,\n",
    "        }\n",
    "        \n",
    "        resp = _snowflake.send_snow_api_request(\n",
    "            \"POST\",\n",
    "            ANALYST_API_ENDPOINT,\n",
    "            {}, {}, request_body, {}, 60000\n",
    "        )\n",
    "        \n",
    "        status = resp.get(\"status\")\n",
    "        if status and int(status) >= 400:\n",
    "            error_msg = f\"Cortex Analyst API error {status}\"\n",
    "            self.messages.pop()  # Remove failed message\n",
    "            raise Exception(error_msg)\n",
    "        \n",
    "        body = resp.get(\"content\", \"\")\n",
    "        if isinstance(body, (bytes, bytearray)):\n",
    "            body = body.decode(\"utf-8\", errors=\"replace\")\n",
    "        \n",
    "        result = json.loads(body)\n",
    "        request_id = resp.get(\"headers\", {}).get(\"X-Snowflake-Request-Id\", \"\")\n",
    "        \n",
    "        # Store analyst response in conversation history\n",
    "        if \"message\" in result:\n",
    "            self.messages.append({**result[\"message\"], \"request_id\": request_id})\n",
    "        \n",
    "        # Extract context for retrieval span\n",
    "        context = self._extract_context(result)\n",
    "        \n",
    "        return {\n",
    "            \"raw_response\": result,\n",
    "            \"request_id\": request_id,\n",
    "            \"context\": context,\n",
    "        }\n",
    "    \n",
    "    def _extract_context(self, api_response: dict) -> List[str]:\n",
    "        \"\"\"Extract interpretation and SQL as retrieval context.\"\"\"\n",
    "        contexts = []\n",
    "        content = api_response.get(\"message\", {}).get(\"content\", [])\n",
    "        \n",
    "        for item in content:\n",
    "            if item.get(\"type\") == \"text\":\n",
    "                contexts.append(f\"Interpretation: {item.get('text', '')}\")\n",
    "            elif item.get(\"type\") == \"sql\":\n",
    "                contexts.append(f\"SQL: {item.get('statement', '')}\")\n",
    "        \n",
    "        return contexts if contexts else [\"No context generated\"]\n",
    "    \n",
    "    def _extract_sql(self, api_response: dict) -> Optional[str]:\n",
    "        \"\"\"Extract SQL statement from API response.\"\"\"\n",
    "        content = api_response.get(\"message\", {}).get(\"content\", [])\n",
    "        for item in content:\n",
    "            if item.get(\"type\") == \"sql\":\n",
    "                return item.get(\"statement\")\n",
    "        return None\n",
    "    \n",
    "    def _extract_interpretation(self, api_response: dict) -> str:\n",
    "        \"\"\"Extract interpretation text from API response.\"\"\"\n",
    "        content = api_response.get(\"message\", {}).get(\"content\", [])\n",
    "        for item in content:\n",
    "            if item.get(\"type\") == \"text\":\n",
    "                return item.get(\"text\", \"\")\n",
    "        return \"No interpretation\"\n",
    "    \n",
    "    def execute_sql(self, sql: str) -> str:\n",
    "        \"\"\"Execute SQL and return results as markdown.\"\"\"\n",
    "        if not sql:\n",
    "            return \"No SQL to execute\"\n",
    "        \n",
    "        try:\n",
    "            df = self.session.sql(sql).to_pandas()\n",
    "            return df.to_markdown(index=False)\n",
    "        except Exception as e:\n",
    "            return f\"SQL execution error: {str(e)}\"\n",
    "    \n",
    "    @instrument(span_type=SpanAttributes.SpanType.GENERATION)\n",
    "    def generate_summary(self, query: str, sql_results: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate human-readable summary from SQL results using Cortex Complete.\n",
    "        This is the GENERATION step - LLM summarization.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Summarize the following SQL query results into a clear, human-readable response.\n",
    "\n",
    "Original Question: {query}\n",
    "\n",
    "SQL Results:\n",
    "{sql_results}\n",
    "\n",
    "Provide a concise summary that directly answers the original question.\"\"\"\n",
    "\n",
    "        payload = {\n",
    "            \"model\": SUMMARIZATION_MODEL,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes data query results clearly and concisely.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 500,\n",
    "        }\n",
    "        \n",
    "        resp = _snowflake.send_snow_api_request(\n",
    "            \"POST\",\n",
    "            COMPLETE_API_ENDPOINT,\n",
    "            {}, {}, payload, {}, 60000\n",
    "        )\n",
    "        \n",
    "        status = resp.get(\"status\")\n",
    "        if status and int(status) != 200:\n",
    "            return f\"Summary generation error: {status}\"\n",
    "        \n",
    "        body = resp.get(\"content\", \"\")\n",
    "        if isinstance(body, (bytes, bytearray)):\n",
    "            body = body.decode(\"utf-8\", errors=\"replace\")\n",
    "        \n",
    "        # Parse response (may be SSE or JSON)\n",
    "        full_content = \"\"\n",
    "        try:\n",
    "            # Try parsing as JSON array (SSE events)\n",
    "            events = json.loads(body)\n",
    "            if isinstance(events, list):\n",
    "                for event in events:\n",
    "                    if \"choices\" in event and event[\"choices\"]:\n",
    "                        delta = event[\"choices\"][0].get(\"delta\", {})\n",
    "                        full_content += delta.get(\"content\", \"\")\n",
    "            else:\n",
    "                # Single JSON response\n",
    "                if \"choices\" in events and events[\"choices\"]:\n",
    "                    full_content = events[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\")\n",
    "        except json.JSONDecodeError:\n",
    "            full_content = body\n",
    "        \n",
    "        return full_content or \"Unable to generate summary\"\n",
    "    \n",
    "    @instrument(\n",
    "        span_type=SpanAttributes.SpanType.RECORD_ROOT,\n",
    "        attributes={\n",
    "            SpanAttributes.RECORD_ROOT.INPUT: \"query\",\n",
    "            SpanAttributes.RECORD_ROOT.OUTPUT: \"return\",\n",
    "        }\n",
    "    )\n",
    "    def answer_query(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Main entry point - returns the generated SQL for comparison with ground truth.\n",
    "        \n",
    "        For SQL comparison evaluation:\n",
    "        - INPUT: User's natural language question\n",
    "        - OUTPUT: Generated SQL query (to compare with expected SQL)\n",
    "        \"\"\"\n",
    "        # Reset conversation for clean comparison\n",
    "        self.reset_conversation()\n",
    "        \n",
    "        # Call Cortex Analyst to get SQL\n",
    "        analyst_result = self.call_cortex_analyst(query)\n",
    "        \n",
    "        # Extract and return the SQL\n",
    "        sql = self._extract_sql(analyst_result[\"raw_response\"])\n",
    "        \n",
    "        if sql:\n",
    "            normalized_sql = self._normalize_sql(sql)\n",
    "            return normalized_sql\n",
    "        else:\n",
    "            return self._extract_interpretation(analyst_result[\"raw_response\"])\n",
    "    \n",
    "    def _normalize_sql(self, sql: str) -> str:\n",
    "        \"\"\"Normalize SQL for fair comparison.\"\"\"\n",
    "        # Remove the Generated by Cortex Analyst comment\n",
    "        sql = re.sub(r'\\s*--\\s*Generated by Cortex Analyst.*', '', sql)\n",
    "        sql = ' '.join(sql.split())\n",
    "        sql = sql.strip().rstrip(';').strip()\n",
    "        return sql\n",
    "    \n",
    "    def reset_conversation(self):\n",
    "        \"\"\"Reset conversation history for new session.\"\"\"\n",
    "        self.messages = []\n",
    "\n",
    "\n",
    "print(\"CortexAnalystApp class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize App and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the app\n",
    "app = CortexAnalystApp(session=session, semantic_view=SEMANTIC_VIEW)\n",
    "print(f\"Cortex Analyst app initialized with semantic view: {SEMANTIC_VIEW}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single query (optional - update the question to match your semantic view)\n",
    "app.reset_conversation()\n",
    "test_response = app.answer_query(\"What is the total count?\")\n",
    "print(f\"Test response: {test_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TruLens Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruSession\n",
    "from trulens.connectors.snowflake import SnowflakeConnector\n",
    "from trulens.apps.app import TruApp\n",
    "from trulens.core.run import Run, RunConfig\n",
    "\n",
    "\n",
    "def setup_evaluation(session, app: CortexAnalystApp):\n",
    "    \"\"\"\n",
    "    Register app and create evaluation run for Snowflake AI Observability.\n",
    "    This will appear under AI & ML -> Evaluations in Snowsight.\n",
    "    \"\"\"\n",
    "    # Create TruLens connector\n",
    "    connector = SnowflakeConnector(snowpark_session=session)\n",
    "    tru_session = TruSession(connector=connector)\n",
    "    \n",
    "    # Register the app\n",
    "    tru_app = TruApp(\n",
    "        app,\n",
    "        app_name=\"CORTEX_ANALYST_SEMANTIC_VIEW\",\n",
    "        app_version=\"v1.0\",\n",
    "        connector=connector,\n",
    "        main_method=app.answer_query,\n",
    "    )\n",
    "    \n",
    "    return tru_app, connector\n",
    "\n",
    "\n",
    "print(\"setup_evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register with TruLens\n",
    "tru_app, connector = setup_evaluation(session, app)\n",
    "print(\"App registered with TruLens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Test Dataset from Snowflake\n",
    "\n",
    "The test dataset table must have these columns:\n",
    "- `INPUT_QUERY`: User questions (natural language)\n",
    "- `OUTPUT_SQL`: Expected SQL queries (ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview test dataset from Snowflake\n",
    "print(f\"Loading test dataset from: {TEST_DATASET_FQN}\")\n",
    "print(\"\")\n",
    "\n",
    "test_df = session.table(TEST_DATASET_FQN).to_pandas()\n",
    "print(f\"Loaded {len(test_df)} test cases\")\n",
    "print(f\"Columns: {list(test_df.columns)}\")\n",
    "print(\"\")\n",
    "print(\"Preview:\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate required columns exist\n",
    "required_columns = [\"INPUT_QUERY\", \"OUTPUT_SQL\"]\n",
    "missing_columns = [col for col in required_columns if col not in test_df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns in test dataset: {missing_columns}\")\n",
    "else:\n",
    "    print(\"Test dataset has all required columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_run(tru_app: TruApp, dataset_table: str, run_name: str = \"semantic_view_eval\"):\n",
    "    \"\"\"\n",
    "    Create an evaluation run with test dataset from Snowflake.\n",
    "    \n",
    "    The dataset table should have columns:\n",
    "    - INPUT_QUERY: User questions (natural language)\n",
    "    - OUTPUT_SQL: Expected SQL queries (ground truth)\n",
    "    \"\"\"\n",
    "    run_config = RunConfig(\n",
    "        run_name=run_name,\n",
    "        description=\"SQL comparison evaluation for Cortex Analyst\",\n",
    "        label=\"cortex_analyst_sql_eval\",\n",
    "        source_type=\"TABLE\",\n",
    "        dataset_name=dataset_table,\n",
    "        dataset_spec={\n",
    "            \"RECORD_ROOT.INPUT\": \"INPUT_QUERY\",\n",
    "            \"RECORD_ROOT.GROUND_TRUTH_OUTPUT\": \"OUTPUT_SQL\",\n",
    "        },\n",
    "        llm_judge_name=JUDGE_MODEL,\n",
    "    )\n",
    "    \n",
    "    run: Run = tru_app.add_run(run_config=run_config)\n",
    "    return run\n",
    "\n",
    "\n",
    "print(\"create_evaluation_run function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation run\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_name = f\"sql_eval_{timestamp}\"\n",
    "run = create_evaluation_run(tru_app, dataset_table=TEST_DATASET_FQN, run_name=run_name)\n",
    "print(f\"Created evaluation run: {run_name}\")\n",
    "print(f\"Using test dataset: {TEST_DATASET_FQN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start evaluation run\n",
    "print(\"Starting evaluation run (this may take a few minutes)...\")\n",
    "run.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check invocation status\n",
    "print(\"Checking invocation status...\")\n",
    "run.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for invocation to complete\n",
    "print(\"Waiting for invocation to complete...\")\n",
    "max_invocation_wait = 120  # 2 minutes\n",
    "poll_interval = 10\n",
    "\n",
    "for i in range(max_invocation_wait // poll_interval):\n",
    "    time.sleep(poll_interval)\n",
    "    print(f\"\\n--- Invocation status check {i+1} ---\")\n",
    "    try:\n",
    "        run.describe()\n",
    "    except Exception as e:\n",
    "        print(f\"Status check error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute evaluation metrics\n",
    "print(\"Computing evaluation metrics...\")\n",
    "print(\"Requesting metrics: correctness, answer_relevance, coherence\")\n",
    "print(\"\\nRequired attributes for these metrics:\")\n",
    "print(\"  - correctness: RECORD_ROOT.INPUT, RECORD_ROOT.OUTPUT, RECORD_ROOT.GROUND_TRUTH_OUTPUT\")\n",
    "print(\"  - answer_relevance: RECORD_ROOT.INPUT, RECORD_ROOT.OUTPUT\")\n",
    "print(\"  - coherence: RECORD_ROOT.OUTPUT\")\n",
    "\n",
    "try:\n",
    "    result = run.compute_metrics([\n",
    "        \"correctness\",         # Compares generated SQL to ground truth SQL\n",
    "        \"answer_relevance\",    # Does the SQL address the question?\n",
    "        \"coherence\",           # Is the output well-structured?\n",
    "    ])\n",
    "    print(f\"\\ncompute_metrics() returned: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR calling compute_metrics(): {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for metrics computation to complete\n",
    "print(\"Metrics are computed asynchronously by Snowflake.\")\n",
    "print(\"This can take 2-5 minutes depending on dataset size.\")\n",
    "print(\"\\nPolling for completion...\")\n",
    "\n",
    "max_wait_minutes = 5\n",
    "poll_interval_seconds = 15\n",
    "max_polls = (max_wait_minutes * 60) // poll_interval_seconds\n",
    "\n",
    "for i in range(max_polls):\n",
    "    time.sleep(poll_interval_seconds)\n",
    "    print(f\"\\n--- Metrics status check {i+1}/{max_polls} (waited {(i+1)*poll_interval_seconds}s) ---\")\n",
    "    try:\n",
    "        run.describe()\n",
    "    except Exception as e:\n",
    "        print(f\"Status check error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DONE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nView results in Snowsight:\")\n",
    "print(\"  AI & ML -> Evaluations -> CORTEX_ANALYST_SEMANTIC_VIEW\")\n",
    "print(f\"\\nRun name: {run_name}\")\n",
    "print(\"\\nIf metrics are still not visible:\")\n",
    "print(\"  1. Refresh the Evaluations page in Snowsight\")\n",
    "print(\"  2. Click on the specific run to see metric details\")\n",
    "print(\"  3. Check that LLM judge model has access permissions\")\n",
    "print(\"\\nFinal run status:\")\n",
    "run.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 1. Setup | Configure semantic view and test dataset location |\n",
    "| 2. App Class | CortexAnalystApp with TruLens instrumentation |\n",
    "| 3. TruLens | Register app with SnowflakeConnector |\n",
    "| 4. Test Data | Load from existing Snowflake table |\n",
    "| 5. Run | Start evaluation run |\n",
    "| 6. Metrics | Compute correctness, answer_relevance, coherence |\n",
    "| 7. Results | View in Snowsight AI & ML -> Evaluations |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
